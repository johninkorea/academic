{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## set seed\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create data\n",
    "N=int(1e2) # number of data\n",
    "x=np.linspace(-.9,.9,N)\n",
    "y=np.sin(x)\n",
    "\n",
    "idx_train=(np.random.choice(N,80,replace=0))\n",
    "idx_test = np.setdiff1d(np.arange(N), idx_train)\n",
    "\n",
    "x_train=x[idx_train]\n",
    "y_train=y[idx_train]\n",
    "\n",
    "x_test=x[idx_test]\n",
    "y_test=y[idx_test]\n",
    "\n",
    "plt.scatter(x_train,y_train,s=1,c='r')\n",
    "plt.scatter(x_test,y_test,s=1,c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning with cpu\n"
     ]
    }
   ],
   "source": [
    "## device choice\n",
    "device = 'mps' if torch.backends.mps.is_built()  else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "# # GPU 사용 가능일 경우 랜덤 시드 고정\n",
    "# if device == 'mps':\n",
    "#     # torch.backends.mps.manual_seed_all(seed)\n",
    "#     torch.backends.mps.is_available()\n",
    "\n",
    "print(\"learning with\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 정의\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, num1, num2):\n",
    "        self.num1 = num1 # nodes per hidden layer\n",
    "        self.num2 = num2 # number of hidden layer\n",
    "\n",
    "        super(NN, self).__init__()\n",
    "        self.layer_in = nn.Sequential(\n",
    "            nn.Linear(1,self.num1),\n",
    "            nn.ReLU())\n",
    "        self.layer_hidden = nn.Sequential(\n",
    "            nn.Linear(self.num1,self.num1),\n",
    "            nn.ReLU())\n",
    "        self.layer_out = nn.Sequential(\n",
    "            nn.Linear(self.num1,1),\n",
    "            # nn.ReLU()) # 이거로 하면 바로 0으로 가서 학습하는게 의미 없어짐\n",
    "            nn.Sigmoid()) # 20번 학습하면 1e-9 order로 수렴함\n",
    "\n",
    "        self.hidden=nn.ModuleList()\n",
    "        for i in range(self.num2):\n",
    "            self.hidden.append(self.layer_hidden)\n",
    "        \n",
    "        \n",
    "        # 가중치를 고르게하는 건데 이 잘 안돼네....\n",
    "        # nn.init.xavier_uniform_(self.layer1.weight) # 지금까지는 대충 설정했는데\n",
    "        # nn.init.xavier_uniform_(self.layer2.weight) # 지금까지는 대충 설정했는데\n",
    "\n",
    "    def forward(self, x): # 순서대로 대입해서 출력하는 것뿐 \n",
    "        out = self.layer_in(x)\n",
    "        for layer in self.hidden:\n",
    "            out = layer(out)\n",
    "        out = self.layer_out(out)\n",
    "        return out\n",
    "\n",
    "# model = NN(3,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model define\n",
    "nodes_per_hidden_layer=5\n",
    "number_of_hidden_layer=2\n",
    "model = NN(nodes_per_hidden_layer,number_of_hidden_layer).to(device)\n",
    "\n",
    "# set hyper parameter\n",
    "lr=1e-2\n",
    "epochs=20\n",
    "batch_size = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = torch.nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# array to tensor\n",
    "train_data = torch.Tensor(x_train)\n",
    "test_data = torch.Tensor(x_test)\n",
    "train_label = torch.LongTensor(y_train)\n",
    "test_label = torch.LongTensor(y_test)\n",
    "\n",
    "# 데이커와 라벨 묶기\n",
    "ds_train = TensorDataset(train_data, train_label)\n",
    "ds_test = TensorDataset(test_data, test_label)\n",
    "\n",
    "# 베치 나누기\n",
    "train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "test = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "total_batch = len(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 1x5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m Y \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m hypothesis \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print(hypothesis)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# print(Y)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m cost \u001b[39m=\u001b[39m criterion(hypothesis\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32), Y\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch3/lib/python3.9/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb Cell 7\u001b[0m in \u001b[0;36mNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x): \u001b[39m# 순서대로 대입해서 출력하는 것뿐 \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_in(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yararal/1code/academic/main/1.hyperparameter_gen/1_nn_macgpu.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         out \u001b[39m=\u001b[39m layer(out)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch3/lib/python3.9/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch3/lib/python3.9/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 1x5)"
     ]
    }
   ],
   "source": [
    "## train\n",
    "costh=[]\n",
    "for epoch in range(epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in train: # 미니 배치 단위로 꺼내온다. X는 이미지, Y는 레이블.\n",
    "        # image is already size of (28x28), no reshape\n",
    "        # label is not one-hot encoded\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        # print(hypothesis)\n",
    "        # print(Y)\n",
    "        cost = criterion(hypothesis.to(torch.float32), Y.to(torch.float32))\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "    # print(avg_cost.item())\n",
    "    # print(type(avg_cost))\n",
    "    # avg_cost=avg_cost.detach().numpy()\n",
    "    avg_cost=avg_cost.item()\n",
    "    print('[Epoch: {:>2}] cost = {:>.6}'.format(epoch + 1, avg_cost))\n",
    "    # os.system(f\"say {epoch + 1} epoch done {epoch + 1} epoch done\")\n",
    "    costh.append(avg_cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs),costh)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"./gpu_model_save.pth\")\n",
    "# model_c = torch.load(\"./gpu_model_save.pth\", map_location=device)\n",
    "# model_c.eval() # 모델 활성화!\n",
    "# print(train_data[1])\n",
    "\n",
    "\n",
    "\n",
    "# yy=[]\n",
    "# z=0\n",
    "# while z<len(train_data):\n",
    "#     model(train_data[z])\n",
    "#     z+=1\n",
    "# plt.plot(train_data,yy)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40811b77fc361d43600929cc59d905b06e0cebb6488c8210251e2e33ec87a75f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
